#!/bin/bash
#COBALT -t 720
#COBALT -n 2
#COBALT -q full-node
#COBALT -A SuperBERT
#COBALT -O bert_pretrain_$jobid
##COBALT -M {YOUR EMAIL}
#COBALT --jobname bert-pretrain

# USAGE:
#   
#   Run locally on a compute node:
#
#     $ ./run_pretraining.cobalt 
#
#   Submit as a Cobalt job (edit Cobalt arguments in the #COBALT directives
#   at the top of the script):
#
#     $ qsub-gpu path/to/run_pretraining.cobalt
#
#   Notes: 
#     - training configuration (e.g., # nodes, # gpus / node, etc.) will be
#       automatically inferred
#     - additional arguments to run_pretraining.py can be specified by
#       including them after run_pretraining.cobalt. E.g.,
#
#       $ ./run_pretraining.cobalt --steps 1000 --learning_rate 5e-4
#

CONFIG="config/deepspeed/bert_pretraining_phase1_config.json"
MODEL_CONFIG="config/deepspeed/bert_large_uncased_config.json"

# Figure out training environment
if [[ -z "${COBALT_NODEFILE}" ]]; then
    RANKS=$HOSTNAME
    NNODES=1
else
    MASTER_RANK=$(head -n 1 $COBALT_NODEFILE)
    RANKS=$(tr '\n' ' ' < $COBALT_NODEFILE)
    NNODES=$(< $COBALT_NODEFILE wc -l)
fi

# Commands to run prior to the Python script for setting up the environment
PRELOAD="source /etc/profile ; "
PRELOAD+="module load conda/pytorch ; "
PRELOAD+="conda activate /lus/theta-fs0/projects/SuperBERT/jgpaul/envs/deepspeed ; "
PRELOAD+="export OMP_NUM_THREADS=8 ; "
PRELOAD+="export TORCH_EXTENSIONS_DIR=/lus/theta-fs0/projects/SuperBERT/jgpaul/DeepSpeed/torch-extensions ; "
#PRELOAD+="export CUDA_LAUNCH_BLOCKING=1 ; "

# torchrun launch configuration
LAUNCHER="python -m torch.distributed.run "
LAUNCHER+="--nnodes=$NNODES --nproc_per_node=auto --max_restarts 0 "
if [[ "$NNODES" -eq 1 ]]; then
    LAUNCHER+="--standalone "
else
    LAUNCHER+="--rdzv_backend=c10d --rdzv_endpoint=$MASTER_RANK "
fi

# Training script and parameters
CMD="run_pretraining_deepspeed.py "
CMD+="--config_file $CONFIG --model_config_file $MODEL_CONFIG"

FULL_CMD=" $PRELOAD $LAUNCHER $CMD $@ "
echo "Training Command: $FULL_CMD"

# Launch the pytorch processes on each worker (use ssh for remote nodes)
RANK=0
for NODE in $RANKS; do
    if [[ "$NODE" == "$HOSTNAME" ]]; then
        echo "Launching rank $RANK on local node $NODE"
        eval $FULL_CMD &
    else
        echo "Launching rank $RANK on remote node $NODE"
        ssh $NODE "cd $PWD; $FULL_CMD" &
    fi
    RANK=$((RANK+1))
done

wait

